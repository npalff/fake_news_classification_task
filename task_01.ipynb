{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake News Detector\n",
    "##### O objetivo desta tarefa é praticar suas habilidades de codificação criando um sistema muito interessante: um Detector de Notícias Falsas. Aqui você tem um trecho de código que irá guiá-lo através do desenvolvimento de todo o sistema. Você só precisará alterar a parte 2. 'FakeNews' class\n",
    "\n",
    "### Boa sorte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Nome do Estudante: \n",
    "##### Estudante ID:\n",
    "##### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Inicio\n",
    "#### (você não precisa alterar essa parte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#no inicio do codigo python, geralmente você adiciona todas as bibliotecas que ira usar durante o script.\n",
    "# para isso basta usar \"import nome_da_biblioteca\"\n",
    "\n",
    "import pandas as pd\n",
    "#panda é usado para manipualação de dados.\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import svm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "#sklearn é uma biblioteca de machine learning, e é usada para criar os modelos nesseários \n",
    "#para a classificação entre fake/true news\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "#nltk  = natural language tool kit. é uma biblioteca para manipualao de texto.\n",
    "\n",
    "import sys, gzip, json, datetime\n",
    "import random\n",
    "import numpy as np\n",
    "import re\n",
    "import string \n",
    "import math\n",
    "from scipy.sparse import vstack\n",
    "#biblioteca secundarias para manipulacao de data,valores numericos,string...\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# usado para plotar graficos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de linhas=4713\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Claim</th>\n",
       "      <th>Claim_ID</th>\n",
       "      <th>Credibility</th>\n",
       "      <th>Description</th>\n",
       "      <th>Example</th>\n",
       "      <th>Fact Check</th>\n",
       "      <th>Google Results</th>\n",
       "      <th>Last Updated</th>\n",
       "      <th>Originally Published</th>\n",
       "      <th>Origins</th>\n",
       "      <th>Referred Links</th>\n",
       "      <th>Tags</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Meijer is offering $100 off \"Back to School\" c...</td>\n",
       "      <td>100-meijer-coupon</td>\n",
       "      <td>false</td>\n",
       "      <td>While each of these scams feature slight varia...</td>\n",
       "      <td>[Collected via e-mail, August 2015] Meijer cou...</td>\n",
       "      <td>Is Meijer offering $100 off \"Back to School\" c...</td>\n",
       "      <td>[{u'results': [{u'domain': u'www.mlive.com', u...</td>\n",
       "      <td>24 August 2015</td>\n",
       "      <td>24 August 2015</td>\n",
       "      <td>In August 2015, a survey scam promising custom...</td>\n",
       "      <td>https://www.facebook.com/meijer/photos/pb.9025...</td>\n",
       "      <td>meijer;meijer coupon;coupon;coupon scam;survey...</td>\n",
       "      <td>www.snopes.com/100-meijer-coupon/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>A photograph depicts a real (giant) pizza, par...</td>\n",
       "      <td>1000-pizza-challenge</td>\n",
       "      <td>true</td>\n",
       "      <td>However, that linked image led only to a \"comi...</td>\n",
       "      <td>[Collected via e-mail, August 2015] I've seen ...</td>\n",
       "      <td>Does a photograph show a real (giant) pizza, p...</td>\n",
       "      <td>[{u'results': [{u'domain': u'www.snopes.com', ...</td>\n",
       "      <td>5 August 2015</td>\n",
       "      <td>5 August 2015</td>\n",
       "      <td>In August 2015, the above-displayed image (of ...</td>\n",
       "      <td>http://www.bigmamaspizza.com/franchisee/coming...</td>\n",
       "      <td>pizza;giant pizza;pizza challenge;competitive ...</td>\n",
       "      <td>www.snopes.com/1000-pizza-challenge/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Image depicts a 102 lb. shrimp caught near Hom...</td>\n",
       "      <td>102-lb-shrimp</td>\n",
       "      <td>false</td>\n",
       "      <td>The image shows a man proudly posing with gian...</td>\n",
       "      <td>[Collected via Facebook, November 2015]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{u'results': [{u'domain': u'www.snopes.com', ...</td>\n",
       "      <td>27 November 2015</td>\n",
       "      <td>27 November 2015</td>\n",
       "      <td>On 6 November 2015, a Facebook user shared the...</td>\n",
       "      <td>http://www.livescience.com/47718-giant-alien-s...</td>\n",
       "      <td>102 lb shrimp;homosassa;fishing;</td>\n",
       "      <td>www.snopes.com/102-lb-shrimp/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>A 14-year-old girl became pregnant after recei...</td>\n",
       "      <td>14-year-old-pregnant-flu-shot</td>\n",
       "      <td>false</td>\n",
       "      <td>She had all the typical symptoms of a pregnant...</td>\n",
       "      <td>[Collected via e-mail, October 2015] I have co...</td>\n",
       "      <td>Did a 14-year-old girl become pregnant after r...</td>\n",
       "      <td>[{u'results': [{u'domain': u'worldnewsdailyrep...</td>\n",
       "      <td>5 October 2015</td>\n",
       "      <td>5 October 2015</td>\n",
       "      <td>On 23 September 2015 the web site World News D...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fake news;world news daily report;WNDR;flu sho...</td>\n",
       "      <td>www.snopes.com/14-year-old-pregnant-flu-shot/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>NASA confirmed there will be 15 straight days ...</td>\n",
       "      <td>15-days-darkness-november</td>\n",
       "      <td>false</td>\n",
       "      <td>Astronomers from NASA have indicated that the ...</td>\n",
       "      <td>[Collected via e-mail, December 2015] I can't ...</td>\n",
       "      <td>Has NASA confirmed that there will be 15 strai...</td>\n",
       "      <td>[{u'results': [{u'domain': u'newswatch33.com',...</td>\n",
       "      <td>4 December 2015</td>\n",
       "      <td>22 July 2015</td>\n",
       "      <td>On 14 July 2015, the web site Newswatch33 publ...</td>\n",
       "      <td>http://newswatch28.com/;https://www.nasa.gov/a...</td>\n",
       "      <td>nasa;newswatch33;huzlers;15 days of darkness;</td>\n",
       "      <td>www.snopes.com/15-days-darkness-november/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              Claim  \\\n",
       "0           0  Meijer is offering $100 off \"Back to School\" c...   \n",
       "1           1  A photograph depicts a real (giant) pizza, par...   \n",
       "2           2  Image depicts a 102 lb. shrimp caught near Hom...   \n",
       "3           3  A 14-year-old girl became pregnant after recei...   \n",
       "4           4  NASA confirmed there will be 15 straight days ...   \n",
       "\n",
       "                        Claim_ID Credibility  \\\n",
       "0              100-meijer-coupon       false   \n",
       "1           1000-pizza-challenge        true   \n",
       "2                  102-lb-shrimp       false   \n",
       "3  14-year-old-pregnant-flu-shot       false   \n",
       "4      15-days-darkness-november       false   \n",
       "\n",
       "                                         Description  \\\n",
       "0  While each of these scams feature slight varia...   \n",
       "1  However, that linked image led only to a \"comi...   \n",
       "2  The image shows a man proudly posing with gian...   \n",
       "3  She had all the typical symptoms of a pregnant...   \n",
       "4  Astronomers from NASA have indicated that the ...   \n",
       "\n",
       "                                             Example  \\\n",
       "0  [Collected via e-mail, August 2015] Meijer cou...   \n",
       "1  [Collected via e-mail, August 2015] I've seen ...   \n",
       "2            [Collected via Facebook, November 2015]   \n",
       "3  [Collected via e-mail, October 2015] I have co...   \n",
       "4  [Collected via e-mail, December 2015] I can't ...   \n",
       "\n",
       "                                          Fact Check  \\\n",
       "0  Is Meijer offering $100 off \"Back to School\" c...   \n",
       "1  Does a photograph show a real (giant) pizza, p...   \n",
       "2                                                NaN   \n",
       "3  Did a 14-year-old girl become pregnant after r...   \n",
       "4  Has NASA confirmed that there will be 15 strai...   \n",
       "\n",
       "                                      Google Results      Last Updated  \\\n",
       "0  [{u'results': [{u'domain': u'www.mlive.com', u...    24 August 2015   \n",
       "1  [{u'results': [{u'domain': u'www.snopes.com', ...     5 August 2015   \n",
       "2  [{u'results': [{u'domain': u'www.snopes.com', ...  27 November 2015   \n",
       "3  [{u'results': [{u'domain': u'worldnewsdailyrep...    5 October 2015   \n",
       "4  [{u'results': [{u'domain': u'newswatch33.com',...   4 December 2015   \n",
       "\n",
       "  Originally Published                                            Origins  \\\n",
       "0       24 August 2015  In August 2015, a survey scam promising custom...   \n",
       "1        5 August 2015  In August 2015, the above-displayed image (of ...   \n",
       "2     27 November 2015  On 6 November 2015, a Facebook user shared the...   \n",
       "3       5 October 2015  On 23 September 2015 the web site World News D...   \n",
       "4         22 July 2015  On 14 July 2015, the web site Newswatch33 publ...   \n",
       "\n",
       "                                      Referred Links  \\\n",
       "0  https://www.facebook.com/meijer/photos/pb.9025...   \n",
       "1  http://www.bigmamaspizza.com/franchisee/coming...   \n",
       "2  http://www.livescience.com/47718-giant-alien-s...   \n",
       "3                                                NaN   \n",
       "4  http://newswatch28.com/;https://www.nasa.gov/a...   \n",
       "\n",
       "                                                Tags  \\\n",
       "0  meijer;meijer coupon;coupon;coupon scam;survey...   \n",
       "1  pizza;giant pizza;pizza challenge;competitive ...   \n",
       "2                   102 lb shrimp;homosassa;fishing;   \n",
       "3  fake news;world news daily report;WNDR;flu sho...   \n",
       "4      nasa;newswatch33;huzlers;15 days of darkness;   \n",
       "\n",
       "                                             URL  \n",
       "0              www.snopes.com/100-meijer-coupon/  \n",
       "1           www.snopes.com/1000-pizza-challenge/  \n",
       "2                  www.snopes.com/102-lb-shrimp/  \n",
       "3  www.snopes.com/14-year-old-pregnant-flu-shot/  \n",
       "4      www.snopes.com/15-days-darkness-november/  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Nosso dataset de noticias falsas estão no arquivo mpi_snopes.csv. Então vamos usar o pandas para abrir esse arquivo \n",
    "# e usar o comando head para ver as primeiras linhas desta dataset. \n",
    "\n",
    "dataset= pd.read_csv(\"mpi_snopes.csv\",encoding=\"utf-8\")\n",
    "print \"Numero de linhas=\"+str(len(dataset))\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Banco de funções\n",
    "###### Aqui você vai encontrar algumas funções básicas, que serão úteis durante a criação da classe 'FakeNews', abaixo.\n",
    "###### não precisa alterar nada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'false', 0.76068702906962715]\n",
      "[(3888, u'The U.S. government is opening gas stations to distribute free gasoline in poor neighborhoods.', 0.51882202323508442), (4099, u'Florida motorists can block the DMV from disclosing their motor vehicle and driver license records.', 0.48975879258509247), (1663, u'Urban Outfitters offered an item appearing to be a bloodstained Kent State sweatshirt for sale.', 0.21190524362869273), (3172, u'Chrysler dealerships owned by Republican donors were disproportionately targeted for closure at the behest of the Obama administration.', 0.19708275992147156), (1835, u'Someone who has taken LSD more than seven times is automatically deemed legally insane.', 0.1737216891217683)]\n"
     ]
    }
   ],
   "source": [
    "#algumas variaveis referentes ao modelo de classificação entre fake/true news.\n",
    "y = dataset.Credibility.values\n",
    "X_text = dataset.Claim.values\n",
    "tfidfVectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5, analyzer='word',stop_words='english')\n",
    "tfidfVectorizer.fit(X_text)\n",
    "X = tfidfVectorizer.transform(X_text) \n",
    "model = svm.SVC(kernel='linear', C=1,probability=True).fit(X, y)\n",
    "\n",
    "\n",
    "#essa função é usada para predizer se uma Afirmação(claim) é verdadeira ou falsa. Por exemplo, 'Global warming has stopped'. \n",
    "#retorno desta função é uma tupla, contendo 1) true/false; 2) probabilidade desta classe ser true/false;\n",
    "#Entrada: vc pode apenas pode alterar a claim(string). \n",
    "#saida: vector of tuplas do tipo (conclusao:string, probabilidade:float)\n",
    "def predictCredibility(claim,model,tfidfVectorizer):\n",
    "    prediction_score=np.max(model.predict_proba(tfidfVectorizer.transform([claim])))\n",
    "    prediction=model.predict(tfidfVectorizer.transform([claim]))[0]\n",
    "    return [prediction,prediction_score]\n",
    "#exemplo, vamos predizer se \"Global warming has stopped\" é verdade ou mentira.\n",
    "print predictCredibility(\"Global warming has stopped\",model,tfidfVectorizer)\n",
    "\n",
    "\n",
    "#essa funcao é usada para dada uma Afirmações(claim), a funcao ira pesquisa no dataset, e retornar K entradas similares.\n",
    "#Entrada: vc pode apenas pode alterar o numero K(numerico), e claim(string). \n",
    "#saida: é um vetor de tuplas. contendo 1) posição da Afirmação(claim) no nosso dataset; 2) A Claim em formato string; 3) a semelhança com a entrada\n",
    "def getMostSimiarClaims(claim,tfidfVectorizer,X_text,X,quant):\n",
    "    vectors_ = vstack((tfidfVectorizer.transform([claim]),X))\n",
    "    #np.append(tfidfVectorizer.transform([claim]),X)\n",
    "    similirity=[]\n",
    "    for v in vectors_:\n",
    "        #computing cosine similarity\n",
    "        similirity.append(cosine_similarity(vectors_[0],v)[0][0])\n",
    "    index = np.array(similirity).argsort()[-quant-1:][::-1][-quant:]\n",
    "    return zip(index,X_text[index],np.array(similirity)[index])\n",
    "#exemplo, obter 5 claims mais parecidas com \"Global warming has stopped\" do nosso dataset\n",
    "print getMostSimiarClaims(\"Global warming has stopped\",tfidfVectorizer, X_text,X,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 'FakeNews' class\n",
    "#### Essa é a parte que você deve alterar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Usando as funções previamente definidas no acima, complete a classe a seguir. Essa classe deve ter métodos:\n",
    "#1) setModel(model)\n",
    "#Esse metodo ira ser reponsavel por receber nosso modelo, previamente definido. \n",
    "#2) setClaim(claim)\n",
    "#Esse método simples para definir o a afirmação (claim). Você deve pegar a entrada, e difinir self.claim. Ex. self.claim=claim\n",
    "#3)predictCredibilityOfClaim(claim)\n",
    "#esse método deve ser reponsavel por dizer para o usuario se essa Afirmação(claim) é verdadeira ou falsa, e qual sua probabilidade.\n",
    "#4)getTopKMostRelatedArticles(k)\n",
    "#esse método é reponsavel por retornar para o usuario quais são as Afirmação(claim) mais parecidas.\n",
    "\n",
    "\n",
    "class FakeNews:\n",
    "    def __init__(self):\n",
    "        self.model=None\n",
    "        self.claim=None\n",
    "\n",
    "    def setModel(self, model):\n",
    "        #esse metodo já está pronta ;)\n",
    "        self.model = model\n",
    "        return True\n",
    "    \n",
    "    def setClaim(self, claim):\n",
    "        #aqui completar, definindo self.claim com a entrada deste metodo.\n",
    "        return True\n",
    "\n",
    "    def predictCredibilityOfClaim(self):\n",
    "        # aqui você vai usar o metodo predictCredibility. Exemplo: predictCredibility(self.claim,self.model,tfidfVectorizer)\n",
    "        # Retorno deve ser uma string, exemplo: \"Essa afirmação parece ser \"+str(prediction[0])+ \" com uma probabilidade de \"+str(prediction[1])\n",
    "        return True\n",
    "    \n",
    "    def getTopKMostRelatedArticles(self,K):\n",
    "        #aqui você vai usar o metodo getMostSimiarClaims. Exemplo: getMostSimiarClaims(self.claim,tfidfVectorizer, X_text,X,5)\n",
    "        #Retorno deve ser uma string, exemplo: \"Existem 5 afirmação parecidas no nosso banco de dados: 1) blablabla (%semelhante), 2)... ...\n",
    "        return True\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Test 'FakeNews' class\n",
    "### A classe que você acabou deverá ser capaz de responder corretamente retornar um valor para os seguintes métodos e entreadas. Se ela está respondendo, então está correto.\n",
    "##### Aqui você não precisa alterar nada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "fake_news_detector = FakeNews()\n",
    "fake_news_detector.setModel(model)\n",
    "fake_news_detector.setClaim(\"Hurricane Harvey brought a shark ashore in Houston\")\n",
    "print fake_news_detector.predictCredibilityOfClaim()\n",
    "print fake_news_detector.getTopKMostRelatedArticles(5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
